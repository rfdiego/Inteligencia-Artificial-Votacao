# -*- coding: utf-8 -*-
"""Cópia de PresidenteEUATree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XnfEe2sCLJ7fwMFkDZbQ1HI6Hqp-WfqR
"""

#Referenciando as dependências
import pandas as pd 
import numpy as np 
from sklearn import tree 
import sklearn.metrics as metrics
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import graphviz 
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()  # estilo do gráfico

# Carregamos o banco de dados 
eua = pd.read_csv('https://raw.githubusercontent.com/rfdiego/Inteligencia-Artificial-Votacao/main/president_county_candidate.csv', sep=',')
datasetoriginal = eua #copiamos o dataset para usarmos de referencia nas features e classes
eua

# informações das variaveis 
eua.info()

#transformando a variavel target em numerica 
labelencoder_X = LabelEncoder()
eua['NCandidate'] = labelencoder_X.fit_transform(eua['candidate'])
eua['NCandidate'].value_counts()
#12 = Jo Jorgensen        
#19 = Donald Trump                 
#20 = Joe Biden                    
#15 = Howie Hawkins                
#1  = Write-ins                  
#34 = Rocky De La Fuente           
#11 = Don Blankenship              
#6  = Brock Pierce                 
#25 = Kanye West                   
#14 = Gloria La Riva               
#5  = Brian Carroll                 
#2  = Alyson Kennedy                
#29 = Phil Collins                  
#27 = Blake Huber                   
#4  = Kyle Kopitke                  
#17 = Jerome Segal                  
#26 = Zachary Scalf                 
#13 = Brooke Paige                  
#8  = Richard Duncan                
#7  = Gary Swing                    
#32 = Christopher LaFontaine        
#37 = Keith McCormic                
#3  = Bill Hammons                  
#16 = Jade Simmons                  
#10 = Dario Hunter                  
#33 = Ricki Sue King                 
#21 = Joe McHugh                     
#9  = John Richard Myers             
#22 = Connie Gammon                  
#31 = Tom Hoefling                   
#36 = Mark Charles                   
#23 = Joseph Kishore                 
#24 = Jordan Scott                   
#28 = Princess Jacob-Fambro          
#30 = President Boddie               
#18 = Jesse Ventura                  
#35 = Sheila Samm Tittle             
#0  = None of these candidates

atributos_continuos = ['votes']
atributos_categoricos = ['state']
for col in atributos_categoricos:
    dummies = pd.get_dummies(eua[col], prefix=col)
    eua = pd.concat([eua, dummies], axis=1)
    eua.drop(col, axis=1, inplace=True)
eua.head()

# Eliminando as colunas repetidas e não numericas
eua.drop(['candidate','party','county'],axis=1,inplace=True)

# informações das novas variaveis 
eua.info()
myfeatures = eua

# separando o treino e o teste
X_train, X_test, y_train, y_test = train_test_split( eua.drop('NCandidate',axis=1), #pega todos valores menos candidatos
                                                     eua['NCandidate'], test_size=0.25) #pega somente os candidatos
clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=8)
clf = clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

print("\nMatriz de confusão detalhada:\n",
      pd.crosstab(y_test, predictions, rownames=['Real'], colnames=['Predito'],
      margins=True, margins_name='Todos'))

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

#decidir qual melhor valor do nivel da arvore
for max_depth in range(1, 20):
    t = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)
    scores = cross_val_score(t, X_test, y_test, cv=5)
    print("Max depth: %d, Accuracy: %0.2f (+/- %0.2f)" % (max_depth, scores.mean(), scores.std()*2))

#eua.drop('NCandidate',axis=1)
#eua['NCandidate']
#ND = eua.drop('NCandidate',axis=1)
#eua.head()
print(list(myfeatures.drop('NCandidate',axis=1)))

dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("eua")
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names= list(myfeatures.drop('NCandidate',axis=1)),
                                
                         class_names= datasetoriginal['candidate'],  
                         filled=True, rounded=True, 
                         special_characters=True)
graph = graphviz.Source(dot_data, format="png") 
graph

#37 = Keith McCormic                 
#36 = Mark Charles 
#35 = Sheila Samm Tittle
#34 = Rocky De La Fuente 
#33 = Ricki Sue King 
#32 = Christopher LaFontaine  
#31 = Tom Hoefling                   
#30 = President Boddie                                      
#29 = Phil Collins                  
#28 = Princess Jacob-Fambro  
#27 = Blake Huber   
#26 = Zachary Scalf  
#25 = Kanye West 
#24 = Jordan Scott                      
#23 = Joseph Kishore              
#22 = Connie Gammon  
#21 = Joe McHugh                 
#20 = Joe Biden    
#19 = Donald Trump 
#18 = Jesse Ventura
#17 = Jerome Segal    
#16 = Jade Simmons                                       
#15 = Howie Hawkins                
#14 = Gloria La Riva
#13 = Brooke Paige
#12 = Jo Jorgensen  
#11 = Don Blankenship                 
#10 = Dario Hunter  
#9  = John Richard Myers 
#8  = Richard Duncan                
#7  = Gary Swing  
#6  = Brock Pierce  
#5  = Brian Carroll                 
#4  = Kyle Kopitke      
#3  = Bill Hammons 
#2  = Alyson Kennedy 
#1  = Write-ins          
#0  = None of these candidates

#['None of these candidates', 'Write-ins','Alyson Kennedy','Bill Hammons','Kyle Kopitke',
#'Brian Carroll','Brock Pierce','Gary Swing','Richard Duncan','John Richard Myers',
#'Dario Hunter','Don Blankenship','Jo Jorgensen','Brooke Paige','Gloria La Riva',
#'Howie Hawkins','Jade Simmons','Jerome Segal','Jesse Ventura','Donald Trump','Joe Biden',
#'Joe McHugh','Connie Gammon','Joseph Kishore','Jordan Scott','Kanye West','Zachary Scalf',
#'Blake Huber','Princess Jacob-Fambro','Phil Collins','President Boddie','Tom Hoefling',
#'Christopher LaFontaine','Ricki Sue King','Rocky De La Fuente','Sheila Samm Tittle',
#'Mark Charles','Keith McCormic']

#01 Variáveis utilizadas: ['state', 'candidate', 'votes'] 34% acuracy
#Referenciando as dependências
import pandas as pd 
import numpy as np 
from sklearn import tree 
import sklearn.metrics as metrics
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import graphviz 
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()  # estilo do gráfico

# Carregamos o banco de dados 
eua = pd.read_csv('https://raw.githubusercontent.com/rfdiego/Inteligencia-Artificial-Votacao/main/president_county_candidate.csv', sep=',')
datasetoriginal = eua #copiamos o dataset para usarmos de referencia nas features e classes
eua

# informações das variaveis 
eua.info()

#transformando a variavel target em numerica 
labelencoder_X = LabelEncoder()
eua['NCandidate'] = labelencoder_X.fit_transform(eua['candidate'])
eua['NCandidate'].value_counts()

atributos_continuos = ['votes']
atributos_categoricos = ['state']
for col in atributos_categoricos:
    dummies = pd.get_dummies(eua[col], prefix=col)
    eua = pd.concat([eua, dummies], axis=1)
    eua.drop(col, axis=1, inplace=True)
eua.head()

# Eliminando as colunas repetidas e não numericas
eua.drop(['candidate','party','county'],axis=1,inplace=True)

# informações das novas variaveis 
eua.info()
myfeatures = eua

# separando o treino e o teste
X_train, X_test, y_train, y_test = train_test_split( eua.drop('NCandidate',axis=1), #pega todos valores menos candidatos
                                                     eua['NCandidate'], test_size=0.25) #pega somente os candidatos
clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=8)
clf = clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

print("\nMatriz de confusão detalhada:\n",
      pd.crosstab(y_test, predictions, rownames=['Real'], colnames=['Predito'],
      margins=True, margins_name='Todos'))
	  
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

#decidir qual melhor valor do nivel da arvore
for max_depth in range(1, 20):
    t = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)
    scores = cross_val_score(t, X_test, y_test, cv=5)
    print("Max depth: %d, Accuracy: %0.2f (+/- %0.2f)" % (max_depth, scores.mean(), scores.std()*2))

print(list(myfeatures.drop('NCandidate',axis=1)))
	

dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("eua")
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names= list(myfeatures.drop('NCandidate',axis=1)),
                                
                         class_names= datasetoriginal['candidate'],  
                         filled=True, rounded=True, 
                         special_characters=True)
graph = graphviz.Source(dot_data, format="png") 
graph

#salvar o Gráfico para ilustração
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
import pydotplus

dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = list(myfeatures.drop('NCandidate',axis=1)),class_names=datasetoriginal['candidate'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('my_decision_tree1.png')

#02 Variáveis utilizadas: Todas 93% acuracy
#Referenciando as dependências
import pandas as pd 
import numpy as np 
from sklearn import tree 
import sklearn.metrics as metrics
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import graphviz 
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()  # estilo do gráfico

# Carregamos o banco de dados 
eua = pd.read_csv('https://raw.githubusercontent.com/rfdiego/Inteligencia-Artificial-Votacao/main/president_county_candidate.csv', sep=',')
datasetoriginal = eua #copiamos o dataset para usarmos de referencia nas features e classes
eua

# informações das variaveis 
eua.info()

#transformando a variavel target em numerica 
labelencoder_X = LabelEncoder()
eua['NCandidate'] = labelencoder_X.fit_transform(eua['candidate'])
eua['NCandidate'].value_counts()

atributos_continuos = ['votes']
atributos_categoricos = ['state','county','party']
for col in atributos_categoricos:
    dummies = pd.get_dummies(eua[col], prefix=col)
    eua = pd.concat([eua, dummies], axis=1)
    eua.drop(col, axis=1, inplace=True)
eua.head()

# Eliminando as colunas repetidas
eua.drop(['candidate'],axis=1,inplace=True)

# informações das novas variaveis 
myfeatures = eua
eua.head()


# separando o treino e o teste
X_train, X_test, y_train, y_test = train_test_split( eua.drop('NCandidate',axis=1), #pega todos valores menos candidatos
                                                     eua['NCandidate'], test_size=0.25) #pega somente os candidatos
clf = tree.DecisionTreeClassifier(criterion='entropy') # usar o maximo nivel para melhor resutado, retirando parametro , max_depth=8
clf = clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

print("\nMatriz de confusão detalhada:\n",
      pd.crosstab(y_test, predictions, rownames=['Real'], colnames=['Predito'],
      margins=True, margins_name='Todos'))
	  
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

'''
#decidir qual melhor valor do nivel da arvore
for max_depth in range(1, 20):
    t = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)
    scores = cross_val_score(t, X_test, y_test, cv=5)
    print("Max depth: %d, Accuracy: %0.2f (+/- %0.2f)" % (max_depth, scores.mean(), scores.std()*2))
'''

print(list(myfeatures.drop('NCandidate',axis=1)))
	

dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("eua")
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names= list(myfeatures.drop('NCandidate',axis=1)),
                                
                         class_names= datasetoriginal['candidate'],  
                         filled=True, rounded=True, 
                         special_characters=True)
graph = graphviz.Source(dot_data, format="png") 
graph

#salvar o Gráfico da arvore para ilustração
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
import pydotplus

dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = list(myfeatures.drop('NCandidate',axis=1)),class_names=datasetoriginal['candidate'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('my_decision_tree2.png')

#03 Variáveis utilizadas ['state', 'candidate', 'votes', 'party'] 94% acuracy
#Referenciando as dependências
import pandas as pd 
import numpy as np 
from sklearn import tree 
import sklearn.metrics as metrics
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import graphviz 
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns; sns.set()  # estilo do gráfico

# Carregamos o banco de dados 
eua = pd.read_csv('https://raw.githubusercontent.com/rfdiego/Inteligencia-Artificial-Votacao/main/president_county_candidate.csv', sep=',')
datasetoriginal = eua #copiamos o dataset para usarmos de referencia nas features e classes
eua

# informações das variaveis 
eua.info()

#transformando a variavel target em numerica 
labelencoder_X = LabelEncoder()
eua['NCandidate'] = labelencoder_X.fit_transform(eua['candidate'])
eua['NCandidate'].value_counts()

atributos_continuos = ['votes']
atributos_categoricos = ['state','party']
for col in atributos_categoricos:
    dummies = pd.get_dummies(eua[col], prefix=col)
    eua = pd.concat([eua, dummies], axis=1)
    eua.drop(col, axis=1, inplace=True)
eua.head()

# Eliminando as colunas 
eua.drop(['candidate'],axis=1,inplace=True)
eua.drop(['county'],axis=1,inplace=True)

# informações das novas variaveis 
#eua.info()
myfeatures = eua
eua.head()


# separando o treino e o teste
X_train, X_test, y_train, y_test = train_test_split( eua.drop('NCandidate',axis=1), #pega todos valores menos candidatos
                                                     eua['NCandidate'], test_size=0.25) #pega somente os candidatos
clf = tree.DecisionTreeClassifier(criterion='entropy') # usar o maximo nivel para melhor resutado, retirando parametro , max_depth=8
clf = clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

print("\nMatriz de confusão detalhada:\n",
      pd.crosstab(y_test, predictions, rownames=['Real'], colnames=['Predito'],
      margins=True, margins_name='Todos'))
	  
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

'''
#decidir qual melhor valor do nivel da arvore
for max_depth in range(1, 20):
    t = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)
    scores = cross_val_score(t, X_test, y_test, cv=5)
    print("Max depth: %d, Accuracy: %0.2f (+/- %0.2f)" % (max_depth, scores.mean(), scores.std()*2))
'''

print(list(myfeatures.drop('NCandidate',axis=1)))
	

dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("eua")
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names= list(myfeatures.drop('NCandidate',axis=1)),
                                
                         class_names= datasetoriginal['candidate'],  
                         filled=True, rounded=True, 
                         special_characters=True)
graph = graphviz.Source(dot_data, format="png") 
graph

#salvar o Gráfico para ilustração
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
import pydotplus

dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = list(myfeatures.drop('NCandidate',axis=1)),class_names=datasetoriginal['candidate'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('my_decision_tree3.png')